{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.mobilenetv3 import mobilenet_v3_small\n",
    "from torchvision.models.resnet import resnet18\n",
    "\n",
    "import nni\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def build_mobilenet_v3():\n",
    "    model = mobilenet_v3_small(pretrained=True)\n",
    "    model.classifier[-1] = torch.nn.Linear(1024, 10)\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def build_resnet18():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = torch.nn.Linear(512, 10)\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def prepare_dataloader(batch_size: int = 128):\n",
    "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    train_loader = DataLoader(\n",
    "        datasets.CIFAR10(Path(__file__).parent / 'data', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=True),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        datasets.CIFAR10(Path(__file__).parent / 'data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def prepare_optimizer(model: torch.nn.Module):\n",
    "    optimize_params = [param for param in model.parameters() if param.requires_grad == True]\n",
    "    optimizer = nni.trace(Adam)(optimize_params, lr=0.001)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, training_step,\n",
    "          lr_scheduler: _LRScheduler, max_steps: int, max_epochs: int):\n",
    "    assert max_epochs is not None or max_steps is not None\n",
    "    train_loader, test_loader = prepare_dataloader()\n",
    "    max_steps = max_steps if max_steps else max_epochs * len(train_loader)\n",
    "    max_epochs = max_steps // len(train_loader) + (0 if max_steps % len(train_loader) == 0 else 1)\n",
    "    count_steps = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(max_epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = training_step((data, target), model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count_steps += 1\n",
    "            if count_steps >= max_steps:\n",
    "                acc = evaluate(model, test_loader)\n",
    "                print(f'[Training Epoch {epoch} / Step {count_steps}] Final Acc: {acc}%')\n",
    "                return\n",
    "        acc = evaluate(model, test_loader)\n",
    "        print(f'[Training Epoch {epoch} / Step {count_steps}] Final Acc: {acc}%')\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return 100 * correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def training_step(batch, model: torch.nn.Module):\n",
    "    output = model(batch[0])\n",
    "    loss = F.cross_entropy(output, batch[1])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(__file__).absolute().parents[1]))\n",
    "\n",
    "import torch\n",
    "\n",
    "# from models import (\n",
    "#     build_resnet18,\n",
    "#     prepare_dataloader,\n",
    "#     prepare_optimizer,\n",
    "#     train,\n",
    "#     training_step,\n",
    "#     evaluate,\n",
    "#     device\n",
    "# )\n",
    "\n",
    "from nni.compression.pytorch import TorchEvaluator\n",
    "from nni.compression.pytorch.pruning import SlimPruner\n",
    "# from nni.compression.pytorch import auto_set_denpendency_group_ids\n",
    "from nni.compression.pytorch import ModelSpeedup\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # finetuning resnet18 on Cifar10\n",
    "    model = build_resnet18()\n",
    "    optimizer = prepare_optimizer(model)\n",
    "    train(model, optimizer, training_step, lr_scheduler=None, max_steps=None, max_epochs=10)\n",
    "    _, test_loader = prepare_dataloader()\n",
    "    print('Original model paramater number: ', sum([param.numel() for param in model.parameters()]))\n",
    "    print('Original model after 10 epochs finetuning acc: ', evaluate(model, test_loader), '%')\n",
    "\n",
    "    config_list = [{\n",
    "        'op_types': ['Conv2d','Linear'],\n",
    "        'sparse_ratio': 0.7\n",
    "    }]\n",
    "    dummy_input = torch.rand(8, 3, 224, 224).to(device)\n",
    "    # config_list = auto_set_denpendency_group_ids(model, config_list, dummy_input)\n",
    "    optimizer = prepare_optimizer(model)\n",
    "    evaluator = TorchEvaluator(train, optimizer, training_step)\n",
    "\n",
    "    pruner = SlimPruner(model, config_list, evaluator, training_steps=1000)\n",
    "\n",
    "    _, masks = pruner.compress()\n",
    "    pruner.unwrap_model()\n",
    "\n",
    "    model = ModelSpeedup(model, dummy_input, masks).speedup_model()\n",
    "    print('Pruned model paramater number: ', sum([param.numel() for param in model.parameters()]))\n",
    "    print('Pruned model without finetuning acc: ', evaluate(model, test_loader), '%')\n",
    "\n",
    "    optimizer = prepare_optimizer(model)\n",
    "    train(model, optimizer, training_step, lr_scheduler=None, max_steps=None, max_epochs=10)\n",
    "    _, test_loader = prepare_dataloader()\n",
    "    print('Pruned model after 10 epochs finetuning acc: ', evaluate(model, test_loader), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

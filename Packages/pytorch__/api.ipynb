{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 维度变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from einops import rearrange,repeat,reduce\n",
    "\n",
    "r = 4\n",
    "\n",
    "x = torch.randn((1, 8, 64, 64))\n",
    "b, c, h, w = x.shape\n",
    "# we want a vector of shape 1, 8, 32, 32\n",
    "x1 = rearrange(x, \"b c h w -> b (h w) c\") # shape = [1, 4096, 8]\n",
    "x2 = x.permute(0,2,3,1).view([b, h*w, c])\n",
    "x3 = x.permute(0,2,3,1).reshape([b, h*w, c])\n",
    "\n",
    "print(x1.equal(x2))\n",
    "print(x1.equal(x2))\n",
    "\n",
    "x3 = rearrange(x2, \"b (h w) c -> b c h w\", h=h, w=w)\n",
    "x4 = x2.view([b, h, w, c]).permute(0,3,1,2)\n",
    "x5 = x2.view([b, h, w, c]).permute(0,3,1,2)\n",
    "print(x3.equal(x4))\n",
    "print(x3.equal(x5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 基础操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#卷积操作\n",
    "#kenel_size = 3 padding= 1 改变通道数操作\n",
    "# conv = nn.Sequential(nn.Conv2d(3, 256, kernel_size=3,padding=1))\n",
    "#输入(batch，channel，H_input，W_input)  输出(batch，channel，H_output，W_output)\n",
    "\n",
    "#线性操作\n",
    "liner = nn.Linear(512,512)\n",
    "# #输入(batch，channel，input)  输出(batch，channel，output)\n",
    "\n",
    "# #unfold操作\n",
    "# unfold = nn.Unfold(kernel_size=4, dilation=1, padding=0, stride=5)\n",
    "\n",
    "# #归一化操作\n",
    "# norm = nn.LayerNorm(1024)\n",
    "\n",
    "# #初始化方法\n",
    "# for m in conv.modules():\n",
    "#     if isinstance(m, nn.Conv2d):\n",
    "#         torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "# print(conv.state_dict())\n",
    "\n",
    "\n",
    "x = torch.rand(1,1024,512)\n",
    "out = liner(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from einops import rearrange,repeat,reduce\n",
    "\n",
    "a = torch.tensor([[1,3,4,12],[1,4,6,1]])\n",
    "# b = torch.rand([1,4,6,1])\n",
    "x = a.flatten(0)\n",
    "print(x.shape)\n",
    "# x = x.transpose(1, 2)\n",
    "# print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数\n",
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "cel = nn.CrossEntropyLoss()\n",
    "nll = nn.NLLLoss()\n",
    "logsoftmax = nn.LogSoftmax(dim=1)\n",
    " \n",
    "x = torch.randn(3,3)\n",
    "y_target = torch.tensor([1,2,0])\n",
    " \n",
    "logsoftmax_value = logsoftmax(x)\n",
    "nll_value = nll(logsoftmax_value,y_target)\n",
    "cel_value = cel(x,y_target)\n",
    " \n",
    "print('x',x)\n",
    "print('y_target',y_target)\n",
    "print('logsoftmax_value',logsoftmax_value)\n",
    "print('nll_value',nll_value)\n",
    "print('cel_value',cel_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 保存整个网络：\n",
    "torch.save(net, path1) \n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = torch.load(path1, map_location = device)   \n",
    "# map_location 参数可选，用于在多卡GPU中训练和推理使用的 CUDA 序号不一样的情况\n",
    "\n",
    "\n",
    "### 只保存网络参数\n",
    "torch.save(net.state_dict(),path2)\n",
    "# 模型定义\n",
    "class MyModel(nn.Module):\n",
    "\tdef __init__(self, para): \n",
    "\t\t...\n",
    "\tdef forward(self, x):\n",
    "\t\t...\n",
    "\n",
    "# 需要先加载模型结构\n",
    "model = MyModel(para = para)\n",
    "\n",
    "# 再加载网络参数\n",
    "model.load_state_dict(torch.load(path2, map_location = device))   # map_location 参数可选\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [0, 0,0,1]\n",
    "y_pred = [0, 1,0,0]\n",
    "c = confusion_matrix(y_true, y_pred)\n",
    "print(c)\n",
    "conf_matrix = torch.zeros(2,2)\n",
    "\n",
    "y_true = torch.tensor([[0, 0,0,1]])\n",
    "y_pred = torch.tensor([[0, 1,0,0]])\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,3,4],[2,5,6]])\n",
    "a>2\n",
    "#  a>2 输出符合条件的布尔矩阵\n",
    "a[a>2]\n",
    "#输出符合条件的一维矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵计数\n",
    "import torch\n",
    "import numpy as np\n",
    "def conunt(array,x):\n",
    "    index = array==x\n",
    "    return index.sum()\n",
    "x = torch.randn(2, 2, 512, 512)\n",
    "N, _, h, w = x.shape\n",
    "pred = x.permute(0, 2, 3, 1).reshape(-1, 2).argmax(axis=1).reshape(N, h, w)\n",
    "pred2 = x.argmax(dim=1)\n",
    "pred = pred.numpy()\n",
    "pred2 = pred2.numpy()\n",
    "a  = pred==pred2\n",
    "print(conunt(a,1))\n",
    "# a = conunt(pred==pred2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算数据集的正反例权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class_0 = 0\n",
    "class_1 = 0\n",
    "file_path = r'D:\\software\\Code\\code-file\\image\\mydata\\my_data2\\anno\\train'\n",
    "for p in os.listdir(file_path):\n",
    "    image_p = os.path.join(file_path,p)\n",
    "    image = cv2.imread(image_p,flags=cv2.IMREAD_GRAYSCALE)\n",
    "    class_0 += (np.sum(image==0))\n",
    "    class_1 += (np.sum(image==1))\n",
    "frenquent_0 = class_0/(class_0+class_1)\n",
    "frenquent_1 = class_1/(class_0+class_1)\n",
    "weight_0 = (1/2)/frenquent_0\n",
    "weight_1 = (1/2)/frenquent_1\n",
    "print(\"class 0 : \",class_0)\n",
    "print(\"class 1 : \",class_1)\n",
    "print(\"frenquent 0 : \",frenquent_0)\n",
    "print(\"frenquent 1 : \",frenquent_1)\n",
    "print(\"weight 0 : \",weight_0)\n",
    "print(\"weight 1 : \",weight_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b72bce0f774da0affb1409740e09e5f72c8a559958be0d948f9a4e26f76c5539"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
